#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CSVæ™ºèƒ½æ‹†åˆ†å·¥å…·
åŠŸèƒ½ï¼šæŒ‰å­—æ®µã€æ—¶é—´å‘¨æœŸæ‹†åˆ†CSVæ–‡ä»¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶è‡ªåŠ¨äºŒæ¬¡æ‹†åˆ†
ç‰ˆæœ¬ï¼šv1.3 (ä¼˜åŒ–è¡Œæ•°æ‹†åˆ†é€»è¾‘)
ä½œè€…ï¼šCopilot AI
æ—¥æœŸï¼š2025-01-06
"""

import pandas as pd
import os
import shutil
import fire
import re
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
import warnings
import chardet

warnings.filterwarnings("ignore")


class DateUtils:
    """æ—¥æœŸå¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def detect_date_format(value):
        """æ£€æµ‹æ—¥æœŸæ ¼å¼"""
        patterns = {
            'yyyyMMdd': r'^\d{4}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])$',
            'yyyy-MM-dd': r'^\d{4}[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])$',
            'yyyyMMdd HH:mm:ss': r'^\d{4}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])\s+([01][0-9]|2[0-3]):[0-5][0-9]:[0-5][0-9]$',
            'yyyy-MM-dd HH:mm:ss': r'^\d{4}[-/](0[1-9]|1[0-2])[-/](0[1-9]|[12][0-9]|3[01])\s+([01][0-9]|2[0-3]):[0-5][0-9]:[0-5][0-9]$'
        }
        
        value_str = str(value).strip()
        for format_name, pattern in patterns.items():
            if re.match(pattern, value_str):
                return format_name
        return None
    
    @staticmethod
    def is_date_column(series, threshold=0.8):
        """
        åˆ¤æ–­åˆ—æ˜¯å¦ä¸ºæ—¥æœŸç±»å‹
        threshold: è‡³å°‘å¤šå°‘æ¯”ä¾‹çš„å€¼ç¬¦åˆæ—¥æœŸæ ¼å¼
        """
        non_null_series = series.dropna()
        if len(non_null_series) == 0:
            return False
        
        date_count = sum(1 for x in non_null_series if DateUtils.detect_date_format(x))
        ratio = date_count / len(non_null_series)
        return ratio >= threshold
    
    @staticmethod
    def convert_to_datetime(series):
        """æ™ºèƒ½è½¬æ¢ä¸ºdatetime"""
        try:
            # å°è¯•pandasè‡ªåŠ¨è§£æ
            return pd.to_datetime(series, errors='coerce')
        except:
            # å°è¯•å„ç§æ ¼å¼
            for fmt in ['%Y%m%d', '%Y-%m-%d', '%Y/%m/%d', 
                       '%Y%m%d %H:%M:%S', '%Y-%m-%d %H:%M:%S']:
                try:
                    return pd.to_datetime(series, format=fmt, errors='coerce')
                except:
                    continue
        return None


class FileUtils:
    """æ–‡ä»¶å¤„ç†å·¥å…·ç±»"""
    
    @staticmethod
    def detect_encoding(file_path):
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read(100000))
            return result['encoding']
    
    @staticmethod
    def safe_filename(name):
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨å­—ç¬¦
        name = str(name).strip()
        unsafe_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        for char in unsafe_chars:
            name = name.replace(char, '_')
        return name[:100]  # é™åˆ¶é•¿åº¦
    
    @staticmethod
    def get_csv_files(path, recursive=False):
        """è·å–æ‰€æœ‰CSVæ–‡ä»¶"""
        path_obj = Path(path)
        if path_obj.is_file():
            return [path_obj] if path_obj.suffix.lower() == '.csv' else []
        elif path_obj.is_dir():
            pattern = '**/*.csv' if recursive else '*.csv'
            return list(path_obj.glob(pattern))
        return []


class CSVSplitter:
    """CSVæ‹†åˆ†æ ¸å¿ƒç±»"""
    
    def __init__(self, max_rows=None, output_dir='./split_data', encoding='auto'):
        """
        åˆå§‹åŒ–æ‹†åˆ†å™¨
        
        Args:
            max_rows: å•æ–‡ä»¶æœ€å¤§è¡Œæ•°
                     - None: ä¸æŒ‰è¡Œæ•°æ‹†åˆ†
                     - æ•´æ•°: æŒ‰æŒ‡å®šè¡Œæ•°æ‹†åˆ†
        """
        self.max_rows = max_rows
        self.output_dir = output_dir
        self.encoding = encoding
        self.stats = {
            'total_files': 0,
            'total_rows': 0,
            'output_files': 0,
            'errors': []
        }
    
    def _prepare_output_dir(self):
        """å‡†å¤‡è¾“å‡ºç›®å½•"""
        if os.path.exists(self.output_dir):
            response = input(f"è¾“å‡ºç›®å½• '{self.output_dir}' å·²å­˜åœ¨ï¼Œæ˜¯å¦æ¸…ç©º? (y/n): ")
            if response.lower() == 'y':
                shutil.rmtree(self.output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
    
    def _read_csv(self, file_path):
        """è¯»å–CSVæ–‡ä»¶ï¼ˆè‡ªåŠ¨æ£€æµ‹ç¼–ç ï¼‰"""
        encoding = self.encoding
        if encoding == 'auto':
            encoding = FileUtils.detect_encoding(file_path)
            print(f"  æ£€æµ‹åˆ°ç¼–ç : {encoding}")
        
        try:
            return pd.read_csv(file_path, encoding=encoding, low_memory=False)
        except:
            # å°è¯•å¸¸è§ç¼–ç 
            for enc in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                try:
                    return pd.read_csv(file_path, encoding=enc, low_memory=False)
                except:
                    continue
            raise ValueError(f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}")
    
    def _split_by_size(self, df, base_name, suffix=''):
        """
        æŒ‰è¡Œæ•°æ‹†åˆ†å¤§æ–‡ä»¶
        
        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            suffix: æ–‡ä»¶ååç¼€
        
        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []
        total_rows = len(df)
        
        # ========== å…³é”®ä¿®æ”¹ï¼šæ ¹æ® max_rows å†³å®šæ˜¯å¦æ‹†åˆ† ==========
        if self.max_rows is None or total_rows <= self.max_rows:
            # ä¸æ‹†åˆ†ï¼Œç›´æ¥ä¿å­˜æ•´ä¸ªæ–‡ä»¶
            file_name = f"{base_name}{suffix}.csv"
            file_path = os.path.join(self.output_dir, file_name)
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            output_files.append((file_name, total_rows))
        else:
            # éœ€è¦æŒ‰è¡Œæ•°æ‹†åˆ†
            num_parts = (total_rows // self.max_rows) + (1 if total_rows % self.max_rows > 0 else 0)
            for i in range(num_parts):
                start_idx = i * self.max_rows
                end_idx = min((i + 1) * self.max_rows, total_rows)
                part_df = df.iloc[start_idx:end_idx]
                
                file_name = f"{base_name}{suffix}_part{i+1}.csv"
                file_path = os.path.join(self.output_dir, file_name)
                part_df.to_csv(file_path, index=False, encoding='utf-8-sig')
                output_files.append((file_name, len(part_df)))
        # ========================================================
        
        return output_files
    
    def _classify_fields(self, df, split_fields):
        """åˆ†ç±»å­—æ®µï¼šæ—¥æœŸå­—æ®µå’Œéæ—¥æœŸå­—æ®µ"""
        date_fields = []
        non_date_fields = []
        
        for field in split_fields:
            if field not in df.columns:
                print(f"  âš ï¸  è­¦å‘Š: å­—æ®µ '{field}' ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡")
                continue
            
            if DateUtils.is_date_column(df[field]):
                date_fields.append(field)
                print(f"  âœ“ '{field}' è¯†åˆ«ä¸ºæ—¥æœŸå­—æ®µ")
            else:
                non_date_fields.append(field)
                print(f"  âœ“ '{field}' è¯†åˆ«ä¸ºæ™®é€šå­—æ®µ")
        
        return date_fields, non_date_fields
    
    def _get_period_label(self, period, period_type):
        """è·å–æ—¶é—´å‘¨æœŸæ ‡ç­¾"""
        period_str = str(period)
        if period_type == 'Y':
            return period_str
        elif period_type == 'Q':
            return period_str.replace('Q', '-Q')
        elif period_type == 'M':
            return period_str
        elif period_type == 'D':
            return period_str
        return period_str
    
    def split_single_file(self, file_path, split_fields, time_period='M'):
        """
        æ‹†åˆ†å•ä¸ªCSVæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            split_fields: æ‹†åˆ†å­—æ®µåˆ—è¡¨
            time_period: æ—¶é—´å‘¨æœŸ (Y/Q/M/D)
        """
        print(f"\n{'='*60}")
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        print(f"{'='*60}")
        
        try:
            # è¯»å–æ–‡ä»¶
            df = self._read_csv(file_path)
            total_rows = len(df)
            print(f"  æ€»è¡Œæ•°: {total_rows:,}")
            print(f"  å­—æ®µæ•°: {len(df.columns)}")
            
            # ========== æ˜¾ç¤ºè¡Œæ•°æ‹†åˆ†ç­–ç•¥ ==========
            if self.max_rows is None:
                print(f"  è¡Œæ•°æ‹†åˆ†: âŒ ä¸æ‹†åˆ†ï¼ˆä¿æŒå®Œæ•´ï¼‰")
            else:
                print(f"  è¡Œæ•°æ‹†åˆ†: âœ… å•æ–‡ä»¶æœ€å¤§ {self.max_rows:,} è¡Œ")
            # =====================================
            
            self.stats['total_files'] += 1
            self.stats['total_rows'] += total_rows
            
            # åˆ†ç±»å­—æ®µ
            date_fields, non_date_fields = self._classify_fields(df, split_fields)
            
            if not date_fields and not non_date_fields:
                print("  âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ‹†åˆ†å­—æ®µ")
                return
            
            # åŸºç¡€æ–‡ä»¶å
            base_name = Path(file_path).stem
            output_files = []
            
            # æ‰§è¡Œæ‹†åˆ†é€»è¾‘
            if len(non_date_fields) >= 2:
                # å¤šä¸ªéæ—¥æœŸå­—æ®µï¼šçº§è”æ‹†åˆ†
                print(f"\n  æ‹†åˆ†ç­–ç•¥: çº§è”æ‹†åˆ† {len(non_date_fields)} ä¸ªå­—æ®µ: {non_date_fields}")
                if date_fields:
                    print(f"  é™„åŠ æ—¶é—´å­—æ®µ: '{date_fields[0]}' (å‘¨æœŸ: {time_period})")
                    output_files = self._split_multi_fields_with_date(
                        df, base_name, non_date_fields, date_fields[0], time_period
                    )
                else:
                    output_files = self._split_multi_non_date_fields(
                        df, base_name, non_date_fields
                    )
            
            elif non_date_fields and date_fields:
                # ç»„åˆæ‹†åˆ†ï¼š1ä¸ªéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µ
                print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{non_date_fields[0]}' + '{date_fields[0]}' (æ—¶é—´å‘¨æœŸ: {time_period})")
                output_files = self._split_by_non_date_and_date(
                    df, base_name, non_date_fields[0], date_fields[0], time_period
                )
            
            elif non_date_fields:
                # ä»…æŒ‰éæ—¥æœŸå­—æ®µæ‹†åˆ†
                print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{non_date_fields[0]}'")
                output_files = self._split_by_non_date(df, base_name, non_date_fields[0])
            
            elif date_fields:
                # ä»…æŒ‰æ—¥æœŸå­—æ®µæ‹†åˆ†
                print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{date_fields[0]}' (æ—¶é—´å‘¨æœŸ: {time_period})")
                output_files = self._split_by_date(df, base_name, date_fields[0], time_period)
            
            # è¾“å‡ºç»“æœç»Ÿè®¡
            print(f"\n  âœ… å®Œæˆ! ç”Ÿæˆ {len(output_files)} ä¸ªæ–‡ä»¶:")
            for file_name, rows in output_files:
                print(f"     - {file_name} ({rows:,} è¡Œ)")
                self.stats['output_files'] += 1
        
        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}"
            print(f"  âŒ {error_msg}")
            self.stats['errors'].append(error_msg)
            import traceback
            traceback.print_exc()
    
    def _split_multi_non_date_fields(self, df, base_name, fields, current_suffix='', level=0):
        """
        é€’å½’æŒ‰å¤šä¸ªéæ—¥æœŸå­—æ®µçº§è”æ‹†åˆ†
        
        ç¤ºä¾‹ï¼šfields = ['çœä»½', 'ç¼–ç ']
        - ç¬¬1å±‚ï¼šæŒ‰'çœä»½'æ‹†åˆ† â†’ å¹¿ä¸œã€æµ™æ±Ÿã€æ±Ÿè‹...
        - ç¬¬2å±‚ï¼šæ¯ä¸ªçœä»½ä¸‹æŒ‰'ç¼–ç 'æ‹†åˆ† â†’ å¹¿ä¸œ_A001ã€å¹¿ä¸œ_A002...
        """
        output_files = []
        
        if level >= len(fields):
            # é€’å½’ç»ˆæ­¢ï¼šä¿å­˜æ–‡ä»¶
            files = self._split_by_size(df, base_name, current_suffix)
            return files
        
        current_field = fields[level]
        unique_values = df[current_field].dropna().unique()
        
        indent = "  " * (level + 2)
        print(f"{indent}ç¬¬{level+1}å±‚æ‹†åˆ† ('{current_field}'): æ‰¾åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
        
        for value in tqdm(unique_values, desc=f"{indent}æ‹†åˆ†ä¸­", leave=False):
            sub_df = df[df[current_field] == value]
            safe_value = FileUtils.safe_filename(value)
            new_suffix = f"{current_suffix}_{safe_value}"
            
            # é€’å½’å¤„ç†ä¸‹ä¸€å±‚
            files = self._split_multi_non_date_fields(
                sub_df, base_name, fields, new_suffix, level + 1
            )
            output_files.extend(files)
        
        return output_files
    
    def _split_multi_fields_with_date(self, df, base_name, non_date_fields, date_field, period_type):
        """
        å¤šéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µçš„çº§è”æ‹†åˆ†
        
        ç¤ºä¾‹ï¼š['çœä»½', 'ç¼–ç '] + 'ç»“ç®—æ—¥æœŸ'
        - ç¬¬1å±‚ï¼šæŒ‰'çœä»½'æ‹†åˆ†
        - ç¬¬2å±‚ï¼šæŒ‰'ç¼–ç 'æ‹†åˆ†
        - ç¬¬3å±‚ï¼šæŒ‰'ç»“ç®—æ—¥æœŸ'æ—¶é—´å‘¨æœŸæ‹†åˆ†
        """
        output_files = []
        
        def recursive_split(sub_df, suffix, field_index):
            """é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°"""
            if field_index >= len(non_date_fields):
                # æ‰€æœ‰éæ—¥æœŸå­—æ®µå¤„ç†å®Œæ¯•ï¼ŒæŒ‰æ—¥æœŸæ‹†åˆ†
                sub_df[date_field] = DateUtils.convert_to_datetime(sub_df[date_field])
                sub_df_valid = sub_df.dropna(subset=[date_field])
                
                if len(sub_df_valid) > 0:
                    grouped = sub_df_valid.groupby(sub_df_valid[date_field].dt.to_period(period_type))
                    for period, period_df in grouped:
                        period_label = self._get_period_label(period, period_type)
                        final_suffix = f"{suffix}_{period_label}"
                        files = self._split_by_size(period_df, base_name, final_suffix)
                        output_files.extend(files)
                
                # å¤„ç†æ—¥æœŸä¸ºç©ºçš„æ•°æ®
                sub_df_null = sub_df[sub_df[date_field].isna()]
                if len(sub_df_null) > 0:
                    final_suffix = f"{suffix}_NULL"
                    files = self._split_by_size(sub_df_null, base_name, final_suffix)
                    output_files.extend(files)
                
                return
            
            # æŒ‰å½“å‰å­—æ®µæ‹†åˆ†
            current_field = non_date_fields[field_index]
            unique_values = sub_df[current_field].dropna().unique()
            
            indent = "  " * (field_index + 2)
            print(f"{indent}ç¬¬{field_index+1}å±‚ ('{current_field}'): {len(unique_values)} ä¸ªå€¼")
            
            for value in tqdm(unique_values, desc=f"{indent}æ‹†åˆ†", leave=False):
                value_df = sub_df[sub_df[current_field] == value]
                safe_value = FileUtils.safe_filename(value)
                new_suffix = f"{suffix}_{safe_value}"
                
                # é€’å½’ä¸‹ä¸€å±‚
                recursive_split(value_df, new_suffix, field_index + 1)
        
        # å¼€å§‹é€’å½’æ‹†åˆ†
        recursive_split(df, '', 0)
        return output_files
    
    def _split_by_non_date(self, df, base_name, field):
        """æŒ‰éæ—¥æœŸå­—æ®µæ‹†åˆ†ï¼ˆå•ä¸ªå­—æ®µï¼‰"""
        output_files = []
        unique_values = df[field].dropna().unique()
        
        print(f"     æ‰¾åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")
        
        for value in tqdm(unique_values, desc="     æ‹†åˆ†ä¸­"):
            sub_df = df[df[field] == value]
            safe_value = FileUtils.safe_filename(value)
            suffix = f"_{safe_value}"
            
            files = self._split_by_size(sub_df, base_name, suffix)
            output_files.extend(files)
        
        return output_files
    
    def _split_by_date(self, df, base_name, date_field, period_type):
        """æŒ‰æ—¥æœŸå­—æ®µæ‹†åˆ†"""
        output_files = []
        
        # è½¬æ¢æ—¥æœŸ
        df[date_field] = DateUtils.convert_to_datetime(df[date_field])
        df_valid = df.dropna(subset=[date_field])
        
        if len(df_valid) == 0:
            print("     âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æ—¥æœŸå€¼")
            return output_files
        
        # æŒ‰å‘¨æœŸåˆ†ç»„
        grouped = df_valid.groupby(df_valid[date_field].dt.to_period(period_type))
        print(f"     æ‰¾åˆ° {len(grouped)} ä¸ªæ—¶é—´å‘¨æœŸ")
        
        for period, period_df in tqdm(grouped, desc="     æ‹†åˆ†ä¸­"):
            period_label = self._get_period_label(period, period_type)
            suffix = f"_{period_label}"
            
            files = self._split_by_size(period_df, base_name, suffix)
            output_files.extend(files)
        
        # å¤„ç†æ—¥æœŸä¸ºç©ºçš„æ•°æ®
        df_null = df[df[date_field].isna()]
        if len(df_null) > 0:
            print(f"     å‘ç° {len(df_null)} è¡Œæ—¥æœŸä¸ºç©ºçš„æ•°æ®")
            suffix = "_NULL"
            files = self._split_by_size(df_null, base_name, suffix)
            output_files.extend(files)
        
        return output_files
    
    def _split_by_non_date_and_date(self, df, base_name, non_date_field, date_field, period_type):
        """ç»„åˆæ‹†åˆ†ï¼š1ä¸ªéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µ"""
        output_files = []
        unique_values = df[non_date_field].dropna().unique()
        
        print(f"     ç¬¬ä¸€å±‚æ‹†åˆ†: æ‰¾åˆ° {len(unique_values)} ä¸ª '{non_date_field}' å€¼")
        
        for value in tqdm(unique_values, desc="     ç¬¬ä¸€å±‚æ‹†åˆ†"):
            sub_df = df[df[non_date_field] == value]
            safe_value = FileUtils.safe_filename(value)
            
            # è½¬æ¢æ—¥æœŸ
            sub_df[date_field] = DateUtils.convert_to_datetime(sub_df[date_field])
            sub_df_valid = sub_df.dropna(subset=[date_field])
            
            if len(sub_df_valid) == 0:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ—¥æœŸï¼Œç›´æ¥ä¿å­˜
                suffix = f"_{safe_value}"
                files = self._split_by_size(sub_df, base_name, suffix)
                output_files.extend(files)
                continue
            
            # æŒ‰æ—¥æœŸå‘¨æœŸåˆ†ç»„
            grouped = sub_df_valid.groupby(sub_df_valid[date_field].dt.to_period(period_type))
            
            for period, period_df in grouped:
                period_label = self._get_period_label(period, period_type)
                suffix = f"_{safe_value}_{period_label}"
                
                files = self._split_by_size(period_df, base_name, suffix)
                output_files.extend(files)
            
            # å¤„ç†è¯¥å€¼ä¸‹æ—¥æœŸä¸ºç©ºçš„æ•°æ®
            sub_df_null = sub_df[sub_df[date_field].isna()]
            if len(sub_df_null) > 0:
                suffix = f"_{safe_value}_NULL"
                files = self._split_by_size(sub_df_null, base_name, suffix)
                output_files.extend(files)
        
        return output_files
    
    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print(f"\n{'='*60}")
        print("å¤„ç†å®Œæˆ!")
        print(f"{'='*60}")
        print(f"è¾“å…¥æ–‡ä»¶: {self.stats['total_files']}")
        print(f"æ€»è¡Œæ•°: {self.stats['total_rows']:,}")
        print(f"è¾“å‡ºæ–‡ä»¶: {self.stats['output_files']}")
        print(f"è¾“å‡ºç›®å½•: {os.path.abspath(self.output_dir)}")
        
        if self.stats['errors']:
            print(f"\nâš ï¸  é”™è¯¯ ({len(self.stats['errors'])}):")
            for error in self.stats['errors']:
                print(f"  - {error}")


class CLI:
    """å‘½ä»¤è¡Œæ¥å£"""
    
    def split(self, 
              input,
              split_fields,
              time_period='M',
              max_rows=None,
              output='./split_data',
              recursive=False,
              encoding='auto'):
        """
        æ‹†åˆ†CSVæ–‡ä»¶
        
        Args:
            input: è¾“å…¥æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹è·¯å¾„
            split_fields: æ‹†åˆ†å­—æ®µï¼Œå¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ï¼ˆå¦‚: "æ´¾ä»¶ç½‘ç‚¹ä¸Šçº§,ç»“ç®—æ—¥æœŸ"ï¼‰
            time_period: æ—¶é—´å‘¨æœŸ (Y=å¹´, Q=å­£åº¦, M=æœˆ, D=æ—¥)
            max_rows: å•æ–‡ä»¶æœ€å¤§è¡Œæ•°
                     - ä¸è®¾ç½®: ä¸æŒ‰è¡Œæ•°æ‹†åˆ†
                     - è®¾ç½®ä½†ä¸ç»™å€¼: é»˜è®¤50ä¸‡è¡Œ
                     - è®¾ç½®å…·ä½“å€¼: æŒ‰è¯¥å€¼æ‹†åˆ†
            output: è¾“å‡ºç›®å½•
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
            encoding: æ–‡ä»¶ç¼–ç  (auto/utf-8/gbkç­‰)
        """
        print(f"\nğŸš€ CSVæ™ºèƒ½æ‹†åˆ†å·¥å…· v1.3")
        print(f"{'='*60}")
        print(f"è¾“å…¥è·¯å¾„: {input}")
        print(f"æ‹†åˆ†å­—æ®µ: {split_fields}")
        print(f"æ—¶é—´å‘¨æœŸ: {time_period}")
        
        # ========== ä¿®æ”¹ï¼šå¤„ç† max_rows å‚æ•° ==========
        if max_rows is None:
            print(f"è¡Œæ•°æ‹†åˆ†: âŒ ä¸æ‹†åˆ†")
            actual_max_rows = None
        elif max_rows is True or max_rows == '':
            # fireåº“ï¼š--max-rowsï¼ˆä¸å¸¦å€¼ï¼‰ä¼šè§£æä¸º True
            print(f"è¡Œæ•°æ‹†åˆ†: âœ… é»˜è®¤ 500,000 è¡Œ")
            actual_max_rows = 500000
        else:
            actual_max_rows = int(max_rows)
            print(f"è¡Œæ•°æ‹†åˆ†: âœ… æ¯ {actual_max_rows:,} è¡Œ")
        # =============================================
        
        print(f"è¾“å‡ºç›®å½•: {output}")
        print(f"{'='*60}")
        
        # è§£æå­—æ®µï¼ˆå…¼å®¹å­—ç¬¦ä¸²å’Œå…ƒç»„ï¼‰
        if isinstance(split_fields, str):
            fields = [f.strip() for f in split_fields.split(',')]
        elif isinstance(split_fields, (tuple, list)):
            fields = [str(f).strip() for f in split_fields]
        else:
            fields = [str(split_fields).strip()]
        
        print(f"è§£æåçš„å­—æ®µ: {fields}\n")
        
        # è·å–æ–‡ä»¶åˆ—è¡¨
        csv_files = FileUtils.get_csv_files(input, recursive)
        
        if not csv_files:
            print(f"âŒ é”™è¯¯: åœ¨ '{input}' ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
            return
        
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶\n")
        
        # åˆå§‹åŒ–æ‹†åˆ†å™¨
        splitter = CSVSplitter(max_rows=actual_max_rows, output_dir=output, encoding=encoding)
        splitter._prepare_output_dir()
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for csv_file in csv_files:
            splitter.split_single_file(csv_file, fields, time_period)
        
        # æ‰“å°æ‘˜è¦
        splitter.print_summary()
    
    def list_fields(self, file, encoding='auto'):
        """
        åˆ—å‡ºCSVæ–‡ä»¶çš„æ‰€æœ‰å­—æ®µ
        
        Args:
            file: CSVæ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç 
        """
        print(f"\nğŸ“‹ æ–‡ä»¶å­—æ®µåˆ—è¡¨")
        print(f"{'='*60}")
        print(f"æ–‡ä»¶: {file}")
        
        try:
            if encoding == 'auto':
                encoding = FileUtils.detect_encoding(file)
                print(f"ç¼–ç : {encoding}")
            
            df = pd.read_csv(file, encoding=encoding, nrows=1000)
            
            print(f"\næ€»å­—æ®µæ•°: {len(df.columns)}")
            print(f"{'='*60}")
            
            for i, col in enumerate(df.columns, 1):
                # æ£€æµ‹æ˜¯å¦ä¸ºæ—¥æœŸå­—æ®µ
                is_date = DateUtils.is_date_column(df[col])
                col_type = "ğŸ“… æ—¥æœŸ" if is_date else "ğŸ“ æ™®é€š"
                
                # æ ·ä¾‹å€¼
                sample = df[col].dropna().iloc[0] if len(df[col].dropna()) > 0 else "N/A"
                
                print(f"{i:3d}. {col_type} | {col:30s} | æ ·ä¾‹: {sample}")
        
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")


def main():
    """ä¸»å…¥å£"""
    fire.Fire(CLI)


if __name__ == '__main__':
    main()