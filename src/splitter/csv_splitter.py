"""
CSV æ‹†åˆ†æ ¸å¿ƒç±»
æä¾›æŒ‰å­—æ®µã€æ—¶é—´å‘¨æœŸæ‹†åˆ†CSVæ–‡ä»¶çš„æ ¸å¿ƒåŠŸèƒ½
"""

import os
from tqdm import tqdm
from ..utils.date_utils import DateUtils
from ..utils.file_utils import FileUtils
from ..utils.constants import TIME_PERIOD_DESCRIPTIONS


class CSVSplitter:
    """CSV æ‹†åˆ†æ ¸å¿ƒç±»"""

    def __init__(self, max_rows=None, output_dir='./split_data', encoding='auto', progress_callback=None):
        """
        åˆå§‹åŒ–æ‹†åˆ†å™¨

        Args:
            max_rows: å•æ–‡ä»¶æœ€å¤§è¡Œæ•°
                - None: ä¸æŒ‰è¡Œæ•°æ‹†åˆ†
                - æ•´æ•°: æŒ‰æŒ‡å®šè¡Œæ•°æ‹†åˆ†
            output_dir: è¾“å‡ºç›®å½•
            encoding: æ–‡ä»¶ç¼–ç  ('auto' è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, message) -> None
                - None: ä¸ä½¿ç”¨å›è°ƒï¼ˆCLIæ¨¡å¼ï¼Œä½¿ç”¨tqdmï¼‰
                - å‡½æ•°: GUIæ¨¡å¼ï¼Œé€šè¿‡å›è°ƒå‘é€è¿›åº¦æ›´æ–°
        """
        self.max_rows = max_rows
        self.output_dir = output_dir
        self.encoding = encoding
        self.progress_callback = progress_callback
        self._reset_stats()

    def _reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.stats = {
            'total_files': 0,
            'total_rows': 0,
            'output_files': 0,
            'output_file_list': [],  # è®°å½•å®é™…ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨ (file_name, row_count)
            'errors': [],
        }

    def _emit_progress(self, current, total, message):
        """
        å‘é€è¿›åº¦æ›´æ–°ï¼ˆå…¼å®¹ CLI å’Œ GUIï¼‰

        Args:
            current: å½“å‰è¿›åº¦å€¼
            total: æ€»å€¼
            message: è¿›åº¦æ¶ˆæ¯
        """
        if self.progress_callback:
            self.progress_callback(current, total, message)
        # CLI æ¨¡å¼ï¼štqdm ä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦æ˜¾ç¤º

    def _classify_fields(self, df, split_fields):
        """
        åˆ†ç±»å­—æ®µï¼šæ—¥æœŸå­—æ®µå’Œéæ—¥æœŸå­—æ®µ

        Args:
            df: pandas DataFrame
            split_fields: è¦æ‹†åˆ†çš„å­—æ®µåˆ—è¡¨

        Returns:
            tuple: (date_fields, non_date_fields)
        """
        date_fields = []
        non_date_fields = []

        for field in split_fields:
            if field not in df.columns:
                print(f"  âš ï¸  è­¦å‘Š: å­—æ®µ '{field}' ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡")
                continue

            if DateUtils.is_date_column(df[field]):
                date_fields.append(field)
                print(f"  âœ“ '{field}' è¯†åˆ«ä¸º ğŸ“… æ—¥æœŸå­—æ®µ")
            else:
                non_date_fields.append(field)
                print(f"  âœ“ '{field}' è¯†åˆ«ä¸º ğŸ“ æ™®é€šå­—æ®µ")

        return date_fields, non_date_fields

    def _split_by_size(self, df, base_name, suffix=''):
        """
        æŒ‰è¡Œæ•°æ‹†åˆ†å¤§æ–‡ä»¶

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            suffix: æ–‡ä»¶ååç¼€

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []
        total_rows = len(df)

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‹†åˆ†
        if self.max_rows is None or total_rows <= self.max_rows:
            # ä¸æ‹†åˆ†ï¼Œç›´æ¥ä¿å­˜æ•´ä¸ªæ–‡ä»¶
            file_name = f"{base_name}{suffix}.csv"
            file_path = os.path.join(self.output_dir, file_name)
            FileUtils.write_csv(df, file_path)
            output_files.append((file_name, total_rows))
            # è®°å½•åˆ°ç»Ÿè®¡
            self.stats['output_file_list'].append((file_name, total_rows))
            self.stats['output_files'] += 1
        else:
            # éœ€è¦æŒ‰è¡Œæ•°æ‹†åˆ†
            num_parts = (total_rows // self.max_rows) + (1 if total_rows % self.max_rows > 0 else 0)
            for i in range(num_parts):
                start_idx = i * self.max_rows
                end_idx = min((i + 1) * self.max_rows, total_rows)
                part_df = df.iloc[start_idx:end_idx]

                file_name = f"{base_name}{suffix}_part{i + 1}.csv"
                file_path = os.path.join(self.output_dir, file_name)
                FileUtils.write_csv(part_df, file_path)
                output_files.append((file_name, len(part_df)))
                # è®°å½•åˆ°ç»Ÿè®¡
                self.stats['output_file_list'].append((file_name, len(part_df)))
                self.stats['output_files'] += 1

        return output_files

    def _split_multi_non_date_fields(self, df, base_name, fields, current_suffix='', level=0):
        """
        é€’å½’æŒ‰å¤šä¸ªéæ—¥æœŸå­—æ®µçº§è”æ‹†åˆ†

        ç¤ºä¾‹ï¼šfields = ['çœä»½', 'åŸå¸‚']
        - ç¬¬1å±‚ï¼šæŒ‰'çœä»½'æ‹†åˆ† â†’ å¹¿ä¸œã€æµ™æ±Ÿã€æ±Ÿè‹...
        - ç¬¬2å±‚ï¼šæ¯ä¸ªçœä»½ä¸‹æŒ‰'åŸå¸‚'æ‹†åˆ† â†’ å¹¿ä¸œ_æ·±åœ³ã€å¹¿ä¸œ_å¹¿å·...

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            fields: å­—æ®µåˆ—è¡¨
            current_suffix: å½“å‰åç¼€
            level: å½“å‰å±‚çº§

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []

        if level >= len(fields):
            # é€’å½’ç»ˆæ­¢ï¼šä¿å­˜æ–‡ä»¶
            return self._split_by_size(df, base_name, current_suffix)

        current_field = fields[level]
        unique_values = df[current_field].dropna().unique()

        indent = "  " * (level + 2)
        print(f"{indent}ç¬¬{level + 1}å±‚ ('{current_field}'): æ‰¾åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")

        for value in tqdm(unique_values, desc=f"{indent}æ‹†åˆ†ä¸­", leave=False):
            sub_df = df[df[current_field] == value]
            safe_value = FileUtils.safe_filename(value)
            new_suffix = f"{current_suffix}_{safe_value}"

            # é€’å½’å¤„ç†ä¸‹ä¸€å±‚
            files = self._split_multi_non_date_fields(
                sub_df, base_name, fields, new_suffix, level + 1
            )
            output_files.extend(files)

        return output_files

    def _split_multi_fields_with_date(self, df, base_name, non_date_fields, date_field, period_type):
        """
        å¤šéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µçš„çº§è”æ‹†åˆ†

        ç¤ºä¾‹ï¼š['çœä»½', 'åŸå¸‚'] + 'è®¢å•æ—¥æœŸ'
        - ç¬¬1å±‚ï¼šæŒ‰'çœä»½'æ‹†åˆ†
        - ç¬¬2å±‚ï¼šæŒ‰'åŸå¸‚'æ‹†åˆ†
        - ç¬¬3å±‚ï¼šæŒ‰'è®¢å•æ—¥æœŸ'æ—¶é—´å‘¨æœŸæ‹†åˆ†

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            non_date_fields: éæ—¥æœŸå­—æ®µåˆ—è¡¨
            date_field: æ—¥æœŸå­—æ®µå
            period_type: æ—¶é—´å‘¨æœŸç±»å‹

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []

        def recursive_split(sub_df, suffix, field_index):
            """é€’å½’æ‹†åˆ†è¾…åŠ©å‡½æ•°"""
            if field_index >= len(non_date_fields):
                # æ‰€æœ‰éæ—¥æœŸå­—æ®µå¤„ç†å®Œæ¯•ï¼ŒæŒ‰æ—¥æœŸæ‹†åˆ†
                sub_df[date_field] = DateUtils.convert_to_datetime(sub_df[date_field])
                sub_df_valid = sub_df.dropna(subset=[date_field])

                if len(sub_df_valid) > 0:
                    # åº”ç”¨æ—¶é—´å‘¨æœŸè¿‡æ»¤
                    period_keys = DateUtils.apply_period_filter(sub_df_valid[date_field], period_type)
                    grouped = sub_df_valid.groupby(period_keys)

                    for period_label, period_df in grouped:
                        final_suffix = f"{suffix}_{period_label}"
                        files = self._split_by_size(period_df, base_name, final_suffix)
                        output_files.extend(files)

                # å¤„ç†æ—¥æœŸä¸ºç©ºçš„æ•°æ®
                sub_df_null = sub_df[sub_df[date_field].isna()]
                if len(sub_df_null) > 0:
                    final_suffix = f"{suffix}_NULL"
                    files = self._split_by_size(sub_df_null, base_name, final_suffix)
                    output_files.extend(files)

                return

            # æŒ‰å½“å‰å­—æ®µæ‹†åˆ†
            current_field = non_date_fields[field_index]
            unique_values = sub_df[current_field].dropna().unique()

            indent = "  " * (field_index + 2)
            print(f"{indent}ç¬¬{field_index + 1}å±‚ ('{current_field}'): {len(unique_values)} ä¸ªå€¼")

            for value in tqdm(unique_values, desc=f"{indent}æ‹†åˆ†", leave=False):
                value_df = sub_df[sub_df[current_field] == value]
                safe_value = FileUtils.safe_filename(value)
                new_suffix = f"{suffix}_{safe_value}"

                # é€’å½’ä¸‹ä¸€å±‚
                recursive_split(value_df, new_suffix, field_index + 1)

        # å¼€å§‹é€’å½’æ‹†åˆ†
        recursive_split(df, '', 0)
        return output_files

    def _split_by_non_date(self, df, base_name, field):
        """
        æŒ‰éæ—¥æœŸå­—æ®µæ‹†åˆ†ï¼ˆå•ä¸ªå­—æ®µï¼‰

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            field: å­—æ®µå

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []
        unique_values = df[field].dropna().unique()

        print(f"     æ‰¾åˆ° {len(unique_values)} ä¸ªå”¯ä¸€å€¼")

        for value in tqdm(unique_values, desc="     æ‹†åˆ†ä¸­"):
            sub_df = df[df[field] == value]
            safe_value = FileUtils.safe_filename(value)
            suffix = f"_{safe_value}"

            files = self._split_by_size(sub_df, base_name, suffix)
            output_files.extend(files)

        return output_files

    def _split_by_date(self, df, base_name, date_field, period_type):
        """
        æŒ‰æ—¥æœŸå­—æ®µæ‹†åˆ†

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            date_field: æ—¥æœŸå­—æ®µå
            period_type: æ—¶é—´å‘¨æœŸç±»å‹

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []

        # è½¬æ¢æ—¥æœŸ
        df[date_field] = DateUtils.convert_to_datetime(df[date_field])
        df_valid = df.dropna(subset=[date_field])

        if len(df_valid) == 0:
            print("     âš ï¸  è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æ—¥æœŸå€¼")
            return output_files

        # æŒ‰å‘¨æœŸåˆ†ç»„
        period_keys = DateUtils.apply_period_filter(df_valid[date_field], period_type)
        grouped = df_valid.groupby(period_keys)
        print(f"     æ‰¾åˆ° {len(grouped)} ä¸ªæ—¶é—´å‘¨æœŸ")

        for period_label, period_df in tqdm(grouped, desc="     æ‹†åˆ†ä¸­"):
            suffix = f"_{period_label}"
            files = self._split_by_size(period_df, base_name, suffix)
            output_files.extend(files)

        # å¤„ç†æ—¥æœŸä¸ºç©ºçš„æ•°æ®
        df_null = df[df[date_field].isna()]
        if len(df_null) > 0:
            print(f"     å‘ç° {len(df_null)} è¡Œæ—¥æœŸä¸ºç©ºçš„æ•°æ®")
            suffix = "_NULL"
            files = self._split_by_size(df_null, base_name, suffix)
            output_files.extend(files)

        return output_files

    def _split_by_non_date_and_date(self, df, base_name, non_date_field, date_field, period_type):
        """
        ç»„åˆæ‹†åˆ†ï¼š1ä¸ªéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µ

        Args:
            df: DataFrame
            base_name: åŸºç¡€æ–‡ä»¶å
            non_date_field: éæ—¥æœŸå­—æ®µå
            date_field: æ—¥æœŸå­—æ®µå
            period_type: æ—¶é—´å‘¨æœŸç±»å‹

        Returns:
            list: [(file_name, row_count), ...]
        """
        output_files = []
        unique_values = df[non_date_field].dropna().unique()

        print(f"     ç¬¬ä¸€å±‚æ‹†åˆ†: æ‰¾åˆ° {len(unique_values)} ä¸ª '{non_date_field}' å€¼")

        for value in tqdm(unique_values, desc="     ç¬¬ä¸€å±‚æ‹†åˆ†"):
            sub_df = df[df[non_date_field] == value].copy()
            safe_value = FileUtils.safe_filename(value)

            # è½¬æ¢æ—¥æœŸ
            sub_df[date_field] = DateUtils.convert_to_datetime(sub_df[date_field])
            sub_df_valid = sub_df.dropna(subset=[date_field])

            if len(sub_df_valid) == 0:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ—¥æœŸï¼Œç›´æ¥ä¿å­˜
                suffix = f"_{safe_value}"
                files = self._split_by_size(sub_df, base_name, suffix)
                output_files.extend(files)
                continue

            # æŒ‰æ—¥æœŸå‘¨æœŸåˆ†ç»„
            period_keys = DateUtils.apply_period_filter(sub_df_valid[date_field], period_type)
            grouped = sub_df_valid.groupby(period_keys)

            for period_label, period_df in grouped:
                suffix = f"_{safe_value}_{period_label}"
                files = self._split_by_size(period_df, base_name, suffix)
                output_files.extend(files)

            # å¤„ç†è¯¥å€¼ä¸‹æ—¥æœŸä¸ºç©ºçš„æ•°æ®
            sub_df_null = sub_df[sub_df[date_field].isna()]
            if len(sub_df_null) > 0:
                suffix = f"_{safe_value}_NULL"
                files = self._split_by_size(sub_df_null, base_name, suffix)
                output_files.extend(files)

        return output_files

    def split_by_rows_only(self, file_path):
        """
        åªæŒ‰è¡Œæ•°æ‹†åˆ†CSVæ–‡ä»¶ï¼ˆä¸æŒ‰å­—æ®µæ‹†åˆ†ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        """
        print(f"\n{'=' * 60}")
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        print(f"{'=' * 60}")
        print("  æ‹†åˆ†æ¨¡å¼: æŒ‰è¡Œæ•°æ‹†åˆ†")

        # å‘é€è¿›åº¦ï¼šå¼€å§‹å¤„ç†
        self._emit_progress(0, 100, f"å¼€å§‹å¤„ç†: {file_path}")

        try:
            # è¯»å–æ–‡ä»¶
            self._emit_progress(10, 100, "è¯»å–æ–‡ä»¶...")
            df = FileUtils.read_csv_with_encoding(file_path, encoding=self.encoding, low_memory=False)
            total_rows = len(df)
            print(f"  æ€»è¡Œæ•°: {total_rows:,}")
            print(f"  å­—æ®µæ•°: {len(df.columns)}")

            # å¿…é¡»è®¾ç½® max_rows
            if self.max_rows is None:
                print("  âŒ é”™è¯¯: æŒ‰è¡Œæ•°æ‹†åˆ†æ¨¡å¼å¿…é¡»è®¾ç½® max_rows å‚æ•°")
                self._emit_progress(100, 100, "å¤„ç†å¤±è´¥ï¼šæœªè®¾ç½® max_rows")
                return

            print(f"  è¡Œæ•°æ‹†åˆ†: âœ… å•æ–‡ä»¶æœ€å¤§ {self.max_rows:,} è¡Œ")

            self.stats['total_files'] += 1
            self.stats['total_rows'] += total_rows

            # åŸºç¡€æ–‡ä»¶å
            base_name = FileUtils.get_file_stem(file_path)

            # æ‰§è¡ŒæŒ‰è¡Œæ•°æ‹†åˆ†
            self._emit_progress(30, 100, "å¼€å§‹æ‹†åˆ†...")
            print("\n  æ‹†åˆ†ç­–ç•¥: æŒ‰è¡Œæ•°æ‹†åˆ†ï¼ˆä¸è¿›è¡Œå­—æ®µåˆ†ç±»ï¼‰")

            output_files = self._split_by_size(df, base_name, suffix='')

            # è¾“å‡ºç»“æœç»Ÿè®¡
            self._emit_progress(90, 100, "å®Œæˆæ‹†åˆ†")
            actual_output_count = len(self.stats['output_file_list'])
            print(f"\n  âœ… å®Œæˆ! ç”Ÿæˆ {actual_output_count} ä¸ªæ–‡ä»¶:")
            for file_name, rows in output_files:
                print(f"     - {file_name} ({rows:,} è¡Œ)")

            self._emit_progress(100, 100, f"å®Œæˆ! ç”Ÿæˆ {actual_output_count} ä¸ªæ–‡ä»¶")

        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}"
            print(f"  âŒ {error_msg}")
            self._emit_progress(100, 100, f"é”™è¯¯: {error_msg}")
            self.stats['errors'].append(error_msg)
            import traceback
            traceback.print_exc()

    def split_single_file(self, file_path, split_fields, time_period=None):
        """
        æ‹†åˆ†å•ä¸ªCSVæ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            split_fields: æ‹†åˆ†å­—æ®µåˆ—è¡¨
            time_period: æ—¶é—´å‘¨æœŸ (Y/H/Q/M/HM/D)ï¼ŒNone è¡¨ç¤ºä¸ä½¿ç”¨æ—¶é—´å‘¨æœŸæ‹†åˆ†
        """
        print(f"\n{'=' * 60}")
        print(f"å¤„ç†æ–‡ä»¶: {file_path}")
        print(f"{'=' * 60}")

        # å‘é€è¿›åº¦ï¼šå¼€å§‹å¤„ç†
        self._emit_progress(0, 100, f"å¼€å§‹å¤„ç†: {file_path}")

        try:
            # è¯»å–æ–‡ä»¶
            self._emit_progress(10, 100, "è¯»å–æ–‡ä»¶...")
            df = FileUtils.read_csv_with_encoding(file_path, encoding=self.encoding, low_memory=False)
            total_rows = len(df)
            print(f"  æ€»è¡Œæ•°: {total_rows:,}")
            print(f"  å­—æ®µæ•°: {len(df.columns)}")

            # æ˜¾ç¤ºè¡Œæ•°æ‹†åˆ†ç­–ç•¥
            if self.max_rows is None:
                print("  è¡Œæ•°æ‹†åˆ†: âŒ ä¸æ‹†åˆ†ï¼ˆä¿æŒå®Œæ•´ï¼‰")
            else:
                print(f"  è¡Œæ•°æ‹†åˆ†: âœ… å•æ–‡ä»¶æœ€å¤§ {self.max_rows:,} è¡Œ")

            self.stats['total_files'] += 1
            self.stats['total_rows'] += total_rows

            # åˆ†ç±»å­—æ®µ
            self._emit_progress(20, 100, "åˆ†æå­—æ®µ...")
            date_fields, non_date_fields = self._classify_fields(df, split_fields)

            if not date_fields and not non_date_fields:
                print("  âŒ é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æ‹†åˆ†å­—æ®µ")
                self._emit_progress(100, 100, "å¤„ç†å¤±è´¥ï¼šæ²¡æœ‰æœ‰æ•ˆå­—æ®µ")
                return

            # åŸºç¡€æ–‡ä»¶å
            base_name = FileUtils.get_file_stem(file_path)
            output_files = []

            # æ‰§è¡Œæ‹†åˆ†é€»è¾‘
            self._emit_progress(30, 100, "å¼€å§‹æ‹†åˆ†...")

            # è¾“å‡ºæ—¶é—´å‘¨æœŸè®¾ç½®ä¿¡æ¯
            if time_period:
                period_desc = TIME_PERIOD_DESCRIPTIONS.get(time_period, time_period)
                print(f"  æ—¶é—´å‘¨æœŸ: {period_desc} ({time_period})")
            else:
                print("  æ—¶é—´å‘¨æœŸ: æœªè®¾ç½®ï¼ˆæ—¥æœŸå­—æ®µå°†æŒ‰å”¯ä¸€å€¼æ‹†åˆ†ï¼‰")

            if len(non_date_fields) >= 2:
                # å¤šä¸ªéæ—¥æœŸå­—æ®µï¼šçº§è”æ‹†åˆ†
                print(f"\n  æ‹†åˆ†ç­–ç•¥: çº§è”æ‹†åˆ† {len(non_date_fields)} ä¸ªå­—æ®µ: {non_date_fields}")
                if date_fields and time_period:
                    # æœ‰æ—¶é—´å‘¨æœŸè®¾ç½®æ—¶ï¼Œæ·»åŠ æ—¥æœŸå­—æ®µæ‹†åˆ†
                    print(f"  é™„åŠ æ—¶é—´å­—æ®µ: '{date_fields[0]}' ({TIME_PERIOD_DESCRIPTIONS.get(time_period, time_period)})")
                    output_files = self._split_multi_fields_with_date(
                        df, base_name, non_date_fields, date_fields[0], time_period
                    )
                elif date_fields and not time_period:
                    # æ— æ—¶é—´å‘¨æœŸè®¾ç½®æ—¶ï¼Œå°†æ—¥æœŸå­—æ®µå½“ä½œæ™®é€šå­—æ®µçº§è”æ‹†åˆ†
                    all_fields = non_date_fields + date_fields
                    print(f"  é™„åŠ å­—æ®µ: {date_fields}ï¼ˆæŒ‰å”¯ä¸€å€¼æ‹†åˆ†ï¼‰")
                    output_files = self._split_multi_non_date_fields(
                        df, base_name, all_fields
                    )
                else:
                    output_files = self._split_multi_non_date_fields(
                        df, base_name, non_date_fields
                    )

            elif non_date_fields and date_fields:
                # 1ä¸ªéæ—¥æœŸå­—æ®µ + 1ä¸ªæ—¥æœŸå­—æ®µ
                if time_period:
                    # æœ‰æ—¶é—´å‘¨æœŸè®¾ç½®ï¼šç»„åˆæ‹†åˆ†
                    print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{non_date_fields[0]}' + '{date_fields[0]}' ({TIME_PERIOD_DESCRIPTIONS.get(time_period, time_period)})")
                    output_files = self._split_by_non_date_and_date(
                        df, base_name, non_date_fields[0], date_fields[0], time_period
                    )
                else:
                    # æ— æ—¶é—´å‘¨æœŸè®¾ç½®ï¼šçº§è”æŒ‰å”¯ä¸€å€¼æ‹†åˆ†
                    print(f"\n  æ‹†åˆ†ç­–ç•¥: çº§è”æ‹†åˆ† '{non_date_fields[0]}' + '{date_fields[0]}'ï¼ˆæŒ‰å”¯ä¸€å€¼ï¼‰")
                    output_files = self._split_multi_non_date_fields(
                        df, base_name, [non_date_fields[0], date_fields[0]]
                    )

            elif non_date_fields:
                # ä»…æŒ‰éæ—¥æœŸå­—æ®µæ‹†åˆ†
                print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{non_date_fields[0]}'")
                output_files = self._split_by_non_date(df, base_name, non_date_fields[0])

            elif date_fields:
                # ä»…æŒ‰æ—¥æœŸå­—æ®µæ‹†åˆ†
                if time_period:
                    print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{date_fields[0]}' ({TIME_PERIOD_DESCRIPTIONS.get(time_period, time_period)})")
                    output_files = self._split_by_date(df, base_name, date_fields[0], time_period)
                else:
                    # æ— æ—¶é—´å‘¨æœŸè®¾ç½®ï¼ŒæŒ‰æ—¥æœŸå”¯ä¸€å€¼æ‹†åˆ†
                    print(f"\n  æ‹†åˆ†ç­–ç•¥: æŒ‰ '{date_fields[0]}'ï¼ˆæŒ‰å”¯ä¸€å€¼ï¼‰")
                    output_files = self._split_by_non_date(df, base_name, date_fields[0])

            # è¾“å‡ºç»“æœç»Ÿè®¡
            self._emit_progress(90, 100, "å®Œæˆæ‹†åˆ†")
            # ç¡®ä¿ç»Ÿè®¡æ­£ç¡®ï¼šä½¿ç”¨å®é™…ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨é•¿åº¦
            actual_output_count = len(self.stats['output_file_list'])
            print(f"\n  âœ… å®Œæˆ! ç”Ÿæˆ {actual_output_count} ä¸ªæ–‡ä»¶:")
            for file_name, rows in output_files:
                print(f"     - {file_name} ({rows:,} è¡Œ)")

            self._emit_progress(100, 100, f"å®Œæˆ! ç”Ÿæˆ {actual_output_count} ä¸ªæ–‡ä»¶")

        except Exception as e:
            error_msg = f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}"
            print(f"  âŒ {error_msg}")
            self._emit_progress(100, 100, f"é”™è¯¯: {error_msg}")
            self.stats['errors'].append(error_msg)
            import traceback
            traceback.print_exc()

    def print_summary(self):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print(f"\n{'=' * 60}")
        print("å¤„ç†å®Œæˆ!")
        print(f"{'=' * 60}")
        print(f"è¾“å…¥æ–‡ä»¶: {self.stats['total_files']}")
        print(f"æ€»è¡Œæ•°: {self.stats['total_rows']:,}")
        print(f"è¾“å‡ºæ–‡ä»¶: {self.stats['output_files']}")
        print(f"è¾“å‡ºç›®å½•: {os.path.abspath(self.output_dir)}")

        if self.stats['errors']:
            print(f"\nâš ï¸  é”™è¯¯ ({len(self.stats['errors'])}):")
            for error in self.stats['errors']:
                print(f"  - {error}")
